{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import count\n",
    "import matplotlib.pyplot as plt\n",
    "from metaphone import doublemetaphone\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "import textdistance\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_predict, cross_validate, StratifiedKFold\n",
    "\n",
    "from matplotlib_venn import venn2, venn2_circles, venn2_unweighted\n",
    "from matplotlib_venn import venn3, venn3_circles\n",
    "\n",
    "from metaphone import doublemetaphone\n",
    "import jellyfish\n",
    "\n",
    "import sys\n",
    "# Add the ptdraft folder path to the sys.path list\n",
    "sys.path.append('..')\n",
    "\n",
    "from model import Siamese\n",
    "from process import load_data, load_json_config, str2emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained folder: contrains all of the information about the run. and how the DL model predicted it.\n",
    "trained_folder = \"../results/gru_soundex\"\n",
    "\n",
    "# results path\n",
    "csv_save_path = \"../results/\"\n",
    "\n",
    "# Best Save for the Run:\n",
    "run_name = \"gru_soundex\"\n",
    "DL_thrsholds = [0.778285384,0.79654932,0.794194341,0.783809662,0.772859931]\n",
    "\n",
    "# Config file:\n",
    "config_file = \"../configs/gru_soundex.json\"\n",
    "\n",
    "# Phonetic RF addition\n",
    "phonetic_RF = True\n",
    "\n",
    "# Result Set: {1800s_ln, 1800s_fn or Normal}:\n",
    "result_set = \"Normal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions:\n",
    "def compare_dm1(s1, s2):\n",
    "    return textdistance.levenshtein.normalized_similarity(doublemetaphone(s1)[0],doublemetaphone(s2)[0])\n",
    "\n",
    "def compare_dm2(s1, s2):\n",
    "    return textdistance.levenshtein.normalized_similarity(doublemetaphone(s1)[1],doublemetaphone(s2)[1])\n",
    "\n",
    "# MAX VALUE for RF\n",
    "vowel_max = 6\n",
    "consonant_max = 13\n",
    "character_max = 19\n",
    "\n",
    "def create_RF_features(dataframe):\n",
    "    dataframe['levenshtein'] = [textdistance.levenshtein.normalized_similarity(x, y) for x, y in dataframe[['name_a', 'name_b']].itertuples(index=False)]\n",
    "    dataframe['jaro'] = [textdistance.jaro.normalized_similarity(x, y) for x, y in dataframe[['name_a', 'name_b']].itertuples(index=False)]\n",
    "    dataframe['jaro_winkler'] = [textdistance.jaro_winkler.normalized_similarity(x, y) for x, y in dataframe[['name_a', 'name_b']].itertuples(index=False)]\n",
    "    dataframe['jaccard'] = [textdistance.jaccard.normalized_similarity(x, y) for x, y in dataframe[['name_a', 'name_b']].itertuples(index=False)]\n",
    "    dataframe['sorensen_dice'] = [textdistance.sorensen_dice.normalized_similarity(x, y) for x, y in dataframe[['name_a', 'name_b']].itertuples(index=False)]\n",
    "    dataframe['dm1'] = [compare_dm1(x, y) for x, y in dataframe[['name_a', 'name_b']].itertuples(index=False)]\n",
    "    dataframe['dm2'] = [compare_dm2(x, y) for x, y in dataframe[['name_a', 'name_b']].itertuples(index=False)]\n",
    "    dataframe['vowels_a'] = dataframe['name_a'].apply(lambda x: sum(map(x.count, 'aeiou')))\n",
    "    dataframe['vowels_b'] = dataframe['name_b'].apply(lambda x: sum(map(x.count, 'aeiou')))\n",
    "    dataframe['consonants_a'] = dataframe['name_a'].str.len() - dataframe['vowels_a']\n",
    "    dataframe['consonants_b'] = dataframe['name_b'].str.len() - dataframe['vowels_b']\n",
    "    dataframe['vowels'] = (dataframe['vowels_a'] - dataframe['vowels_b']).abs()\n",
    "    dataframe['vowels'] = 1 - (dataframe['vowels'] / vowel_max)\n",
    "    dataframe['consonants'] = (dataframe['consonants_a'] - dataframe['consonants_b']).abs()\n",
    "    dataframe['consonants'] = 1 - (dataframe['consonants'] / consonant_max)\n",
    "    dataframe['characters'] = (dataframe['name_a'].str.len() - dataframe['name_b'].str.len()).abs()\n",
    "    dataframe['characters'] = 1 - (dataframe['characters'] / character_max)\n",
    "\n",
    "    #Phonetic Component:\n",
    "    dataframe[\"levenshtein_phonetic\"] = [textdistance.levenshtein.normalized_similarity(jellyfish.soundex(x), jellyfish.soundex(y)) for x, y in dataframe[['name_a', 'name_b']].itertuples(index=False)]\n",
    "    dataframe[\"jw_phonetic\"] = [textdistance.jaro_winkler.normalized_similarity(jellyfish.soundex(x), jellyfish.soundex(y)) for x, y in dataframe[['name_a', 'name_b']].itertuples(index=False)]\n",
    "\n",
    "    dataframe = dataframe.drop(columns=['vowels_a', 'vowels_b', 'consonants_a', 'consonants_b'])\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get the dataset and folds from trained folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_a</th>\n",
       "      <th>name_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dessell</td>\n",
       "      <td>pessall</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ellgood</td>\n",
       "      <td>elwood</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ermann</td>\n",
       "      <td>erman</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>koland</td>\n",
       "      <td>nowland</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>radebach</td>\n",
       "      <td>rasbach</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>tildsley</td>\n",
       "      <td>bertini</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>rieck</td>\n",
       "      <td>riek</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>dippery</td>\n",
       "      <td>brohart</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>wipperman</td>\n",
       "      <td>wippermann</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>utteridge</td>\n",
       "      <td>uttridge</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          name_a      name_b  label\n",
       "0        dessell     pessall  False\n",
       "1        ellgood      elwood  False\n",
       "2         ermann       erman   True\n",
       "3         koland     nowland  False\n",
       "4       radebach     rasbach  False\n",
       "...          ...         ...    ...\n",
       "14995   tildsley     bertini  False\n",
       "14996      rieck        riek  False\n",
       "14997    dippery     brohart  False\n",
       "14998  wipperman  wippermann   True\n",
       "14999  utteridge    uttridge   True\n",
       "\n",
       "[15000 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds = []\n",
    "\n",
    "for i in range(5):\n",
    "  test_csv = os.path.join(trained_folder, f\"test_k{i}.csv\")\n",
    "  df = pd.read_csv(test_csv, usecols=[\"name1\", \"name2\", \"label\"])\n",
    "  df = df.astype({\"label\": bool, \"name1\": str, \"name2\": str})\n",
    "  df = df.rename(columns={\"name1\": \"name_a\", \"name2\": \"name_b\"})\n",
    "\n",
    "  folds.append(df)\n",
    "\n",
    "len(folds)\n",
    "\n",
    "folds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train the RF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_precision': 0.9030918765344335, 'test_recall': 0.93076, 'f1_score': 0.9167086101343502, 'tp': 4653.8, 'tn': 9500.6, 'fp': 499.4, 'fn': 346.2}\n"
     ]
    }
   ],
   "source": [
    "out_pairs = []\n",
    "\n",
    "for pairs in folds:\n",
    "    out_pairs.append(create_RF_features(pairs))\n",
    "    \n",
    "folds = out_pairs\n",
    "    \n",
    "scores = defaultdict(list)\n",
    "y_prob = []\n",
    "name_a = []\n",
    "name_b = []\n",
    "labels = []\n",
    "\n",
    "rf_y_pred = []\n",
    "\n",
    "rf_models = []\n",
    "\n",
    "for test_fold_index in range(len(folds)):\n",
    "    val_fold_index = test_fold_index - 1 if test_fold_index - 1 >= 0 else len(folds) - 1\n",
    "\n",
    "    X_train = pd.DataFrame()\n",
    "    y_train = pd.Series(dtype=bool)\n",
    "    for fold_index in range(len(folds)):\n",
    "        if fold_index != test_fold_index and fold_index != val_fold_index:\n",
    "            X_train = pd.concat([X_train, folds[fold_index].drop(columns=['name_a', 'name_b', 'label'])])\n",
    "            y_train = pd.concat([y_train, folds[fold_index]['label']])\n",
    "    name_a = name_a + folds[test_fold_index][\"name_a\"].values.tolist()\n",
    "    name_b = name_b + folds[test_fold_index][\"name_b\"].values.tolist()\n",
    "    X_test = folds[test_fold_index].drop(columns=['name_a', 'name_b', 'label'])\n",
    "    y_test = folds[test_fold_index]['label']\n",
    "    \n",
    "    clf = RandomForestClassifier(random_state=0)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    rf_y_pred += np.array(y_pred).tolist()\n",
    "\n",
    "    scores['test_precision'].append(precision_score(y_test, y_pred))\n",
    "    scores['test_recall'].append(recall_score(y_test, y_pred))\n",
    "    scores['f1_score'].append(f1_score(y_test, y_pred))\n",
    "\n",
    "    yt = np.array(y_test)\n",
    "    yp = np.array(y_pred)\n",
    "\n",
    "    to_labels = yt.astype(int).tolist()\n",
    "    labels = labels + to_labels\n",
    "\n",
    "    scores['tp'].append(np.count_nonzero(yt & yp))\n",
    "    scores['tn'].append(np.count_nonzero(np.logical_not(yt) & np.logical_not(yp)))\n",
    "    scores['fp'].append(np.count_nonzero(np.logical_not(yt) & yp))\n",
    "    scores['fn'].append(np.count_nonzero(yt & np.logical_not(yp)))\n",
    "\n",
    "    y_prob += [x[1] for x in clf.predict_proba(X_test)]\n",
    "\n",
    "    rf_models.append(clf)\n",
    "\n",
    "print({k: mean(v) for k, v in scores.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load the best DF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded run successfully from ../saves/gru_soundex_k0_BEST\n",
      "Loaded run successfully from ../saves/gru_soundex_k1_BEST\n",
      "Loaded run successfully from ../saves/gru_soundex_k2_BEST\n",
      "Loaded run successfully from ../saves/gru_soundex_k3_BEST\n",
      "Loaded run successfully from ../saves/gru_soundex_k4_BEST\n",
      "[Siamese(\n",
      "  (A_function): GRU(36, 25, num_layers=8, batch_first=True, bidirectional=True)\n",
      "  (bidirectional_linear): Linear(in_features=3050, out_features=25, bias=True)\n",
      "), Siamese(\n",
      "  (A_function): GRU(36, 25, num_layers=8, batch_first=True, bidirectional=True)\n",
      "  (bidirectional_linear): Linear(in_features=3050, out_features=25, bias=True)\n",
      "), Siamese(\n",
      "  (A_function): GRU(36, 25, num_layers=8, batch_first=True, bidirectional=True)\n",
      "  (bidirectional_linear): Linear(in_features=3050, out_features=25, bias=True)\n",
      "), Siamese(\n",
      "  (A_function): GRU(36, 25, num_layers=8, batch_first=True, bidirectional=True)\n",
      "  (bidirectional_linear): Linear(in_features=3050, out_features=25, bias=True)\n",
      "), Siamese(\n",
      "  (A_function): GRU(36, 25, num_layers=8, batch_first=True, bidirectional=True)\n",
      "  (bidirectional_linear): Linear(in_features=3050, out_features=25, bias=True)\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "dl_models = []\n",
    "\n",
    "for k in range(5):\n",
    "    save_file = os.path.join(\"../saves\", f\"{run_name}_k{k}_BEST\")\n",
    "\n",
    "    json_file = os.path.join(\"configs\", str(run_name) + \".json\")\n",
    "    DATASET_CONFIG, TRAIN_CONFIG, MODEL_KWARGS = load_json_config(config_file)\n",
    "\n",
    "    _, model, _, _, _, _, _ = load_data(save_file, TRAIN_CONFIG, MODEL_KWARGS)\n",
    "    dl_models.append(model)\n",
    "    \n",
    "print(dl_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Get the Results set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    folds.append(df)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def get_1800s_ln_results_set(path = \"../data/1800s_last_name_pairs.tsv\"):\n",
    "    df = pd.read_csv(path, delimiter = \"\\t\", names = [\"name0\", \"name1\"])\n",
    "    df[\"gt_label\"] = 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_1800s_fn_results_set(path = \"../data/1800s_first_name_pairs.tsv\"):\n",
    "    df = pd.read_csv(path, delimiter = \"\\t\", names = [\"name0\", \"name1\"])\n",
    "    df[\"gt_label\"] = 1\n",
    "    df = df.dropna(axis = 0)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def get_Normal_results_set(path = \"../data/gru_metaphone\"):\n",
    "    folds = []\n",
    "\n",
    "    for i in range(5):\n",
    "        test_csv = os.path.join(trained_folder, f\"test_k{i}.csv\")\n",
    "        df = pd.read_csv(test_csv, usecols=[\"name1\", \"name2\", \"label\"])\n",
    "        df = df.astype({\"label\": bool, \"name1\": str, \"name2\": str})\n",
    "        df = df.rename(columns={\"name1\": \"name0\", \"name2\": \"name1\", \"label\": \"gt_label\"})\n",
    "        df[\"gt_label\"] = df[\"gt_label\"].astype(int)\n",
    "\n",
    "\n",
    "        folds.append(df)\n",
    "\n",
    "    df = pd.concat(folds, axis = 0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "directory = {\"1800s_ln\": get_1800s_ln_results_set, \"1800s_fn\": get_1800s_fn_results_set, \"Normal\": get_Normal_results_set}\n",
    "function = directory[result_set]\n",
    "results_set = function()\n",
    "results_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Run models on result set \n",
    "\n",
    "Creates DL classification (score), DL classification (label), and RF classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'name0': 'dessell', 'name1': 'pessall', 'gt_label': False, 'DL_classification_score': 0.4728097140789032, 'DL_classification_label': 0, 'RF_classification_label': 0}\n",
      "100 {'name0': 'uselman', 'name1': 'usselman', 'gt_label': True, 'DL_classification_score': 0.9599664688110352, 'DL_classification_label': 1, 'RF_classification_label': 1}\n",
      "200 {'name0': 'humphry', 'name1': 'heaslet', 'gt_label': False, 'DL_classification_score': 0.5513315260410309, 'DL_classification_label': 0, 'RF_classification_label': 0}\n",
      "300 {'name0': 'hetrick', 'name1': 'hetzke', 'gt_label': False, 'DL_classification_score': 0.5893280982971192, 'DL_classification_label': 0, 'RF_classification_label': 0}\n",
      "400 {'name0': 'schaurer', 'name1': 'scheuren', 'gt_label': False, 'DL_classification_score': 0.4880423963069916, 'DL_classification_label': 0, 'RF_classification_label': 0}\n",
      "500 {'name0': 'bardson', 'name1': 'maryson', 'gt_label': False, 'DL_classification_score': 0.43894789218902586, 'DL_classification_label': 0, 'RF_classification_label': 0}\n",
      "600 {'name0': 'dobkowski', 'name1': 'dukowski', 'gt_label': False, 'DL_classification_score': 0.44483280181884766, 'DL_classification_label': 0, 'RF_classification_label': 0}\n",
      "700 {'name0': 'cottrell', 'name1': 'jaeke', 'gt_label': False, 'DL_classification_score': 0.47087514996528623, 'DL_classification_label': 0, 'RF_classification_label': 0}\n",
      "800 {'name0': 'caltrider', 'name1': 'hogen', 'gt_label': False, 'DL_classification_score': 0.5051034450531006, 'DL_classification_label': 0, 'RF_classification_label': 0}\n",
      "900 {'name0': 'quirouette', 'name1': 'freddle', 'gt_label': False, 'DL_classification_score': 0.4697277843952179, 'DL_classification_label': 0, 'RF_classification_label': 0}\n",
      "1000 {'name0': 'barsen', 'name1': 'bearson', 'gt_label': False, 'DL_classification_score': 0.7841370105743408, 'DL_classification_label': 0, 'RF_classification_label': 0}\n",
      "1100 {'name0': 'willcocks', 'name1': 'wilcocks', 'gt_label': True, 'DL_classification_score': 0.9598362445831299, 'DL_classification_label': 1, 'RF_classification_label': 1}\n",
      "1200 {'name0': 'roushar', 'name1': 'rusher', 'gt_label': False, 'DL_classification_score': 0.6464955568313598, 'DL_classification_label': 0, 'RF_classification_label': 0}\n",
      "1300 {'name0': 'horp', 'name1': 'sumter', 'gt_label': False, 'DL_classification_score': 0.46304322481155397, 'DL_classification_label': 0, 'RF_classification_label': 0}\n",
      "1400 {'name0': 'whincup', 'name1': 'wincup', 'gt_label': True, 'DL_classification_score': 0.8991553783416748, 'DL_classification_label': 1, 'RF_classification_label': 1}\n",
      "1500 {'name0': 'gillings', 'name1': 'gellings', 'gt_label': True, 'DL_classification_score': 0.9487866520881653, 'DL_classification_label': 1, 'RF_classification_label': 1}\n",
      "1600 {'name0': 'shewmaker', 'name1': 'sleymaker', 'gt_label': False, 'DL_classification_score': 0.5527467668056488, 'DL_classification_label': 0, 'RF_classification_label': 0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-94e3c69b5113>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#print(n1_emb, n2_emb)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn1_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn2_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mscore_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results_dir = results_set.to_dict('records')\n",
    "results_dir_out = list()\n",
    "\n",
    "for index, pair in enumerate(results_dir):\n",
    "    # Deep Learning:\n",
    "    n1_emb = str2emb(pair['name0']).unsqueeze(0)\n",
    "    n2_emb = str2emb(pair['name1']).unsqueeze(0)\n",
    "\n",
    "    score_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for model, threshold in zip(dl_models, DL_thrsholds):\n",
    "        #print(n1_emb, n2_emb)\n",
    "        score, (_, _) = model(n1_emb, n2_emb)\n",
    "        score = score.item()\n",
    "        score_list.append(score)\n",
    "\n",
    "        label = 1 if score > threshold else 0\n",
    "\n",
    "        label_list.append(label)\n",
    "\n",
    "    DL_classification_score = sum(score_list) / 5\n",
    "    DL_classification_label = 1 if label_list.count(1) >= 3 else 0\n",
    "\n",
    "    # Random Forest:\n",
    "    pair_rf_input = {\"name_a\": pair[\"name0\"], \"name_b\": pair[\"name1\"], \"label\": pair[\"gt_label\"]}\n",
    "    input_df = pd.DataFrame(pair_rf_input, index = [0])\n",
    "    rf_features = create_RF_features(input_df)\n",
    "    X_test = rf_features.drop(columns=['name_a', 'name_b', 'label'])\n",
    "\n",
    "    label_list = []\n",
    "\n",
    "    for model in rf_models:\n",
    "        label = model.predict(X_test)\n",
    "        label_list.append(label)\n",
    "\n",
    "    RF_classification_label = 1 if label_list.count(True) >= 3 else 0\n",
    "\n",
    "    out = {\"name0\": pair['name0'], \"name1\": pair['name1'], \"gt_label\": pair['gt_label'], \"DL_classification_score\": DL_classification_score, \\\n",
    "           \"DL_classification_label\": DL_classification_label, \"RF_classification_label\": RF_classification_label}\n",
    "    results_dir_out.append(out)\n",
    "\n",
    "    if index % 100 == 0:\n",
    "        print(index, out)\n",
    "\n",
    "results_set = pd.DataFrame(results_dir_out)\n",
    "results_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Create additional data for the result set\n",
    "DL Bucket, RF bucket, concatenated bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bucket(gt, pred):\n",
    "    if gt == 1 and pred == 1:\n",
    "        return \"tp\"\n",
    "    elif gt == 1 and pred == 0:\n",
    "        return \"fn\"\n",
    "    elif gt == 0 and pred == 1:\n",
    "        return \"fp\"\n",
    "    elif gt == 0 and pred == 0:\n",
    "        return \"tn\"\n",
    "    \n",
    "results_set[\"DL_bucket\"] = [calculate_bucket(x, y) for x, y in results_set[[\"gt_label\", \"DL_classification_label\"]].itertuples(index=False)]\n",
    "results_set[\"RF_bucket\"] = [calculate_bucket(x, y) for x, y in results_set[[\"gt_label\", \"RF_classification_label\"]].itertuples(index=False)]\n",
    "results_set[\"concat_bucket\"] = [x + \"_\" + y for x, y in results_set[[\"DL_bucket\", \"RF_bucket\"]].itertuples(index=False)]\n",
    "\n",
    "results_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Save to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_set.to_csv(os.path.join(csv_save_path, run_name + \"_\" + result_set + \".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
